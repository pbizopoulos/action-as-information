<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta content="width=device-width, initial-scale=1" name="viewport">
		<title>Action as Information</title>
		<link href="data:," rel="icon">
		<link href="https://pbizopoulos.github.io/style.css" rel="stylesheet">
		<link href="style.css" rel="stylesheet">
		<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css" crossorigin="anonymous">
		<script defer src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>
		<script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js" crossorigin="anonymous"></script>
		<script defer src="https://cdn.jsdelivr.net/npm/katex@latest/dist/contrib/auto-render.min.js" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
		<script defer src="https://d3js.org/d3.v7.min.js"></script>
		<script defer src="sanConvEncoder.js"></script>
		<script defer src="sanResize.js"></script>
		<script defer src="noisyOneMotif.js"></script>
		<script defer src="script.js"></script>
	</head>
	<body>
		<div id="div-header">
			<div id="div-navigation">
				<a href="https://pbizopoulos.github.io">Home page</a> / <b>Action as Information</b>
			</div>
			<div id="div-source-code-link">
				<a href="https://github.com/pbizopoulos/action-as-information">Source code</a>
			</div>
		</div>

		<b>Abstract</b>

		<p>
		We define action as proportional to the energy and description length of some data.
		We then present Sparsely Activated Networks (SANs) and an interactive parametric dashboard, that allows exploring action by modifying the data and the model configuration.
		We demonstrate the use of action as a loss function, which results in learning interpretable kernels.
		Lastly, we argue that this definition of action can be used to quantify information and discuss how it relates with previous literature.
		</p>

		<b>Introduction</b>

		<p>
		Action is a functional that receives the history of a system as input and outputs a number.
		In physics, the defining property of action is that it is minimized for the history that nature chose to follow (e.g. the specific path that an object followed when a force is applied to it).
		</p>

		<p>
		The system for which we will define action is data (e.g. a set of numbers such as vectors) and the history is the description of their change; how they 'traversed' from their initial to their final values.
		Action has dimensions of energy &times; time, however in our definition we use description length as a hardware-independent measure of time.
		In other words, action is proportional to the quantity (energy) and duration (description length) of the change of the data.
		</p>

		<p>
		In the physical world, an object followed the one optimal path over the infinite possible paths, which can be found by minimizing the action (i.e. principle of least/stationary action<a href="#citation-1">[1]</a>).
		However data have multiple descriptions, which poses the following questions:
		</p>
		<ul>
			<li>Is there an optimal description for some data? If yes then, how can we approximate it?</li>
			<li>What does the value of the action mean?</li>
		</ul>
		<p>
		We argue that the optimal description is the one that minimizes action, and here we will try to approximate it using Sparsely Activated Networks.
		Then, we argue that action is the quantity of information of the data that is 'perceived' by the description (viewed as an observer).
		</p>

		<b>Formal definition of Action</b>

		<p>
		Let \(x \in \mathbb{R}^n\) some data and \(d_{0 \rightarrow x} \in D\) a description of the traversal from \(0\) to \(x\), where \(D\) a description language.
		We define action \(\mathcal{S}\) of \(x\) w.r.t. \(d_{0 \rightarrow x}\) as:
		$$\mathcal{S} = \big\| x \big\| \big\| d_{0 \rightarrow x} \big\|$$
		</p>

		<p>
		, where \(\big\| \cdot \big\|\) denotes the norm.
		Calculating \(\big\| x \big\|\) is straightforward, however calculating \(\big\| d_{0 \rightarrow x} \big\|\) heavily depends on \(d_{0 \rightarrow x}\), e.g. choosing a neural network as \(d_{0 \rightarrow x}\) we could have the following:
		$$d_{0 \rightarrow x} = W + a + c$$
		, where \(W\) is the number of weights, \(a\) the number of activations when fed with \(x\) and \(c\) a constant that is the description length of the neural network architecture.
		</p>

		<p>
		Let \(\hat{x}\) an approximation of \(x\), then \(\mathcal{S}\) becomes:
		$$\mathcal{S} = \big\| \hat{x} \big\| \big\| d_{0 \rightarrow \hat{x}} \big\| + \big\| x - \hat{x} \big\| \big\| d_{\hat{x} \rightarrow x} \big\|$$
		,where \(d_{0 \rightarrow \hat{x}}\) is the description for \(\hat{x}\), \(\big\| x - \hat{x} \big\|\) the energy of the loss and \(\big\| d_{\hat{x} \rightarrow x} \big\|\) the description length of the loss which could be considered as equal to the length of \(x\) in the worst case.
		This formulation allows us to trade description length with energy and get closer to the minima of \(\mathcal{S}\).
		</p>

		<b>Sparsely Activated Networks</b>

		<p>
		We present Sparsely Activated Networks (SANs)<a href="#citation-2">[2]</a> in order to use them as \(d_{0 \rightarrow \hat{x}}\).
		The reason we use SANs is that it easily allows us modify \(\big\| d_{0 \rightarrow \hat{x}} \big\|\) during training with the following operators (either individually or in combination):
		</p>
		<ul>
			<li>\(T_a(x, c)\): Amplitude threshold function</li>
			<li>\(\phi\): Activation function</li>
			<li>\(T_e(x, c)\): Minimum distance threshold function</li>
		</ul>

		<p>
		During training, we calculate \(\hat{x}\) as follows:
		$$\hat{x} = \sum_{i=1}^m{w_i * T_e(\phi(T_a(w_i * x)))}$$
		, where \(w_i \in \mathbb{R}^{k_i}\) are the \(m\) trainable weights, \(k_i \in \mathbb{N}\) are the kernel lengths and \(*\) is the convolution operator.
		</p>

		<p>
		\(w_i\) are learnt by backpropagating the loss \(L\) w.r.t. to \(w_i\):
		$$\Delta w_i = -\lambda\frac{\partial L}{\partial w_i}$$
		, where \(\lambda\) is the learning rate.
		</p>

		<p>
		For reconstructing (describing) \(x\), we set the activations \(\alpha_i = T_e(\phi(T_a(w_i * x)))\) and use the following:
		$$\hat{x} = \sum_{i=1}^m{w_i * \alpha_i}$$
		</p>

		<p>
		We can finally calculate \(\big\| d_{0 \rightarrow \hat{x}} \big\|\) from the previous:
		$$\big\| d_{0 \rightarrow \hat{x}} \big\| = \max\bigg((\dim(x) + 1)\sum_{i=1}^m{\big\|\alpha_i\big\|_0}, n\bigg) + \sum_{i=1}^m{k_i}$$
		, where \(\dim\) denotes dimensionality, \(\max\) the maximum function and we omit the neural network architecture constant \(c\) as it does not change within our experiment.
		</p>

		<b>Interactive parametric dashboard</b>

		<p>
		Here we demonstrate the calculation of \(\mathcal{S}\) of motif-based time-series w.r.t. SANs as \(d_{0 \rightarrow \hat{x}}\).
		The dashboard consists of four types of blocks indicated by the background color:
		</p>
		<ul>
			<li><svg class="annotationSvg fill-white" width="14" height="14"><rect width="14" height="14" /></svg> SAN architecture with data flow</li>
			<li><svg class="annotationSvg fill-gainsboro" width="14" height="14"><rect width="14" height="14" /></svg> parametric input control panel</li>
			<li><svg class="annotationSvg fill-silver" width="14" height="14"><rect width="14" height="14" /></svg> parametric SAN control panel</li>
			<li><svg class="annotationSvg fill-darkgray" width="14" height="14"><rect width="14" height="14" /></svg> start and reference control panel</li>
		</ul>

		<div class="grid-container" id="gridContainerDiv">
			<div class="border-solid grid-item visibility-hidden" id="inputControlDiv1">
				<div>
					<span>\(n\) = </span><span id="sizeText"></span>
					<input id="sizeInputRange" max="100" min="2" type="range">
				</div>
				<div>
					<div id="velocityText"></div>
					<input id="velocityInputRange" max="20" min="0" type="range">
				</div>
			</div>
			<div class="border-solid grid-item visibility-hidden" id="inputControlDiv2">
				<div>
					<input id="channelUseInputCheckbox" type="checkbox">
					<svg height="14" id="inputChannelMotifColoredBoxSvg" width="14">
						<rect height="14" width="14" />
					</svg>
					<select id="inputChannelIndexSelect"></select>
				</div>
				<div>
					<span>type: </span><select id="inputChannelMotifTypeSelect"></select>
				</div>
				<div>
					<div id="channelMotifSizeText"></div>
					<input type="range" id="channelMotifSizeInputRange" min="1" max="100">
				</div>
				<div>
					<div id="channelAmplitudeBaseText"></div>
					<input id="channelAmplitudeBaseInputRange" max="1" min="-1" step="0.1" type="range">
				</div>
			</div>
			<div class="border-solid grid-item visibility-hidden" id="inputControlDiv3">
				<div>
					<div id="channelAmplitudeMaxText"></div>
					<input type="range" id="channelAmplitudeMaxInputRange" min="-1" step="0.1" max="1">
				</div>
				<div>
					<div id="channelDistanceMinText"></div>
					<input id="channelDistanceMinInputRange" max="100" min="0" type="range" >
				</div>
					<div>
						<div id="channelDistanceMaxText"></div>
						<input id="channelDistanceMaxInputRange" max="100" min="0" type="range" >
					</div>
			</div>
			<div class="border-solid grid-item visibility-hidden" id="inputControlDiv4">
				<div>
					<label>
						dynamic noise:<input id="noiseInitializeInputCheckbox" type="checkbox">
					</label>
				</div>
				<div>
					<span>noise: </span><select id="inputNoiseTypeSelect"></select>
				</div>
				<div>
					<div id="noiseSigmaText"></div>
					<input id="noiseSigmaInputRange" max="1" min="0" step="0.1" type="range">
				</div>
			</div>
			<div class="border-solid grid-item visibility-hidden" id="inputControlDiv5">
				<div>
					<div>resize function: </div>
					<select id="inputResizeFunctionSelect"></select>
				</div>
				<div>
					<div id="resizeMultiplierText"></div>
					<input id="resizeMultiplierInputRange" max="1" min="0.1" step="0.1" type="range">
				</div>
				<div>
					<div id="quantizationStatesNumText"></div>
					<input id="quantizationStatesNumInputRange" max="100" min="1" type="range">
				</div>
				<div>
					<label>
						standardize:<input id="standardizeInputCheckbox" type="checkbox">
					</label>
				</div>
			</div>
			<div class="border-solid grid-item visibility-hidden" id="neuronControlDiv1">
				<div>
					<input id="neuronUseInputCheckbox" type="checkbox">
					<svg height="14" id="neuronColoredBoxSvg" width="14">
						<rect height="14" width="14" />
					</svg>
					<select id="neuronIndexSelect"></select>
				</div>
				<div>
					<span>init: </span><select id="kernelInitializationSelect"></select>
				</div>
				<div>
					<div id="kernelSizeText"></div>
					<input id="kernelSizeInputRange" min="1" type="range">
				</div>
				<div>
					<div id="kernelAmplitudeText"></div>
					<input id="kernelAmplitudeInputRange" max="1" min="-1" step="0.1" type="range">
				</div>
			</div>
			<div class="border-solid grid-item visibility-hidden" id="neuronControlDiv2">
				<div>
					<div>resize function: </div>
					<select id="kernelResizeFunctionSelect"></select>
				</div>
				<div>
					<div id="kernelResizeMultiplierText"></div>
					<input id="kernelResizeMultiplierInputRange" max="100" min="0.1" step="0.1" type="range" >
				</div>
			</div>
			<div class="border-solid grid-item" id="inputDiv"></div>
			<div class="grid-item" id="horizontalLineDiv1"></div>
			<div class="grid-item" id="horizontalLineDiv2"></div>
			<div class="grid-item" id="convEncoderDiv"></div>
			<div class="border-solid grid-item" id="similaritiesDiv"></div>
			<div class="border-solid grid-item visibility-hidden" id="neuronControlDiv3">
				<div>
					<label>
						ConvEncoder:<input id="convEncoderUseInputCheckbox" type="checkbox">
					</label>
				</div>
				<div>
					<div id="kernelStrideText"></div>
					<input id="kernelStrideInputRange" min="1" type="range">
				</div>
				<div>
					<div>resize function: </div>
					<select id="strideResizeFunctionSelect"></select>
				</div>
			</div>
			<div class="border-solid grid-item visibility-hidden" id="lossControlDiv">
				<div>
					<span>optimizer: </span><select id="optimizerSelect"></select>
				</div>
				<div>
					<span>loss: </span><select id="lossFunctionSelect"></select>
				</div>
				<div>
					<div id="learningRateText"></div>
					<input id="learningRateExponentInputRange" max="1" min="-3" step="0.1" type="range">
				</div>
			</div>
			<div class="grid-item" id="lossDiv"></div>
			<div class="border-solid grid-item" id="ndnlDiv"></div>
			<div class="grid-item" id="helpDiv"></div>
			<div class="border-solid grid-item" id="kernelDiv"></div>
			<div class="grid-item" id="activationFunctionDiv"></div>
			<div class="border-solid grid-item" id="activationFunctionControlDiv">
				<div>
					<div id="activationAmplitudeMinText"></div>
					<input id="activationAmplitudeMinInputRange" max="1" min="0" step="0.1" type="range">
				</div>
				<div>
					<span>\(\phi\): </span><select id="activationFunctionSelect"></select>
				</div>
				<div>
					<div id="activationDistanceMinText"></div>
					<input id="activationDistanceMinInputRange" type="range" min="0">
				</div>
				<div>
					<label class="visibility-hidden" id="activationRegulatesLabel">
						regulates:<input type="checkbox" id="activationRegulatesInputCheckbox">
					</label>
				</div>
				<div>
					<label class="visibility-hidden" id="activationRegulatedLabel">
						regulated:<input id="activationRegulatedInputCheckbox" type="checkbox">
					</label>
				</div>
			</div>
			<div class="border-solid grid-item" id="actionDiv">
				<div>
					<span>\(\big\| \hat{x} \big\|\) = </span><span id="inputReconstructionEnergyText"></span>
				</div>
				<div>
					<span>\(\big\| x - \hat{x} \big\|\) = </span><span id="inputReconstructionLossText"></span>
				</div>
				<div>
					<span>\(\big\| d_{0 \rightarrow \hat{x}} \big\|\) = </span><span id="descriptionLengthText"></span>
				</div>
				<div>
					<span>\(\big\| d_{\hat{x} \rightarrow x} \big\|\) = </span><span id="lossDescriptionLengthText"></span>
				</div>
				<div>
					<span>\(\mathcal{S}\) = </span><span id="actionText"></span>
				</div>
			</div>
			<div class="border-solid grid-item" id="reconstructionDiv"></div>
			<div class="grid-item" id="sumDiv"></div>
			<div class="border-solid grid-item" id="kernelReconstructionsDiv"></div>
			<div class="grid-item" id="convDecoderDiv"></div>
			<div class="border-solid grid-item" id="activationsDiv"></div>
			<div class="border-solid grid-item" id="startPauseResetControlDiv">
				<div>
					<button id="startPauseButton" type="button">start</button>
					<button id="stopButton" type="button">stop</button>
				</div>
				<div>
					<div>example: </div>
					<select id="exampleSelect"></select>
				</div>
				<div>
					<div id="epochText"></div>
				</div>
				<div>
					<div id="timePerEpochText"></div>
				</div>
				<div>
					<label>
						advanced:<input type="checkbox" id="advancedInputCheckbox">
					</label>
				</div>
			</div>
			<div class="border-solid grid-item visibility-hidden" id="referenceControlDiv">
				<div>
					<select id="referenceFunctionSelect"></select>
				</div>
				<div>
					<span>\(\big\| \hat{x} \big\|\) = </span><span id="referenceZeroReconstructionLossText"></span>
				</div>
				<div>
					<span>\(\big\| x - \hat{x} \big\|\) = </span><span id="referenceReconstructionLossText"></span>
				</div>
				<div>
					<span>\(\big\| d_{0 \rightarrow \hat{x}} \big\|\) = </span><span id="referenceDescriptionLengthText"></span>
				</div>
				<div>
					<span>\(\big\| d_{\hat{x} \rightarrow x} \big\|\) = </span><span id="referenceLossDescriptionLengthText"></span>
				</div>
				<div>
					<span>\(\mathcal{S}\) = </span><span id="referenceActionText"></span>
				</div>
			</div>
		</div>

		<p>
		We notice that sparse activations that adequately cover the range of \(x\) produce lower \(\mathcal{S}\) and that by modifying \(T_a\) and \(T_e\) the local minima of \(\mathcal{S}\) correspond to more interpretable kernels.
		An easy way to demonstrate the previous is by varying the \(T_e\) slider and inspect the corresponding values of \(\mathcal{S}\).
		</p>

		<b>Discussion</b>

		<p>
		We can view \(\mathcal{S}\) as a deterministic metric of the informational content of \(x\) (observation) that is perceived by \(d_{0 \rightarrow \hat{x}}\) (observer); the lower the value of \(\mathcal{S}\), the less information (or less random) is perceived by \(d_{0 \rightarrow \hat{x}}\).
		</p>

		<p>
		According to previous literature, information is traditionally quantified as proportional to the degree of surprise that is contained in the data<a href="#citation-3">[3]</a>.
		However, this requires assuming the probability distribution of the data generator.
		Kolmogorov-based information (complexity) of some data is defined as the minimum description length, but this is uncomputable<a href="#citation-4">[4]</a> and requires an exact description of the data.
		\(\mathcal{S}\) can be seen as a generalization of Kolmogorov-based information; if we consider \(\hat{x} = x\), then the general problem of minimizing \(\mathcal{S}\) is equivalent with computing Kolmogorov complexity.
		\(\mathcal{S}\) also differs from algorithmic information theory<a href="#citation-5">[5]</a>, since it does not depend on the concept of probability.
		</p>

		<p>
		Practically, \(\mathcal{S}\) minimization can be used for model selection in unsupervised problems and does not require an exact reconstruction like the Minimum Description Length (MDL)<a href="#citation-6">[6]</a>.
		From the same point of view, \(\mathcal{S}\) could be used as a quantification of the bias-variance trade-off.
		</p>

		<p>
		More generally, \(\mathcal{S}\) minimization could be used to answer the question: "Why do we create and use languages?": "Because languages reduce the \(\mathcal{S}\) of our observations.".
		Observers (e.g. biological/silicon brains, sensory organs, measuring instruments) as part of their environment, do not have enough capacity to store their observations.
		To overcome this, a part of their capacity could be used to learn and operate languages (natural, programming, mathematical, computational etc.).
		This allows the observers to describe their observations, in order to make sense of the world and communicate.
		However, this conciseness that languages provide comes with the cost of decreased accuracy.
		For example, the brain internally reconstructs (describes) sensory observations with accuracy constrained by the physiology of each sensory organ (e.g. the human eye cannot distinguish objects less than a tenth of a millimeter).
		Similarly, biological organisms describe concepts or thoughts to varying levels of accuracy constrained by the expressiveness of the language used within an environment.
		In the interactive dashboard, we could view the set of kernels as a generated language, which along with the activations consist of the description of the data.
		</p>

		<p>
		Lastly, \(\mathcal{S}\) is a lossy compression metric and could be viewed as an intelligence metric, which is in contrast with previous literature that assumes intelligence is lossless compression<a href="#citation-7">[7]</a>.
		</p>

		<b>Conclusion</b>

		<p>
		By defining \(\mathcal{S}\) for data and their descriptions we conclude that minimizing \(\mathcal{S}\) provides us with accurate and concise descriptions of data in the same way that action from physics shows as the path that nature chose to take.
		</p>

		<b>References</b>

		<p id="citation-1">[1] Siburg, Karl Friedrich. The principle of least action in geometry and dynamics. No. 1844. Springer Science &amp; Business Media, 2004.</p>
		<p id="citation-2">[2] Bizopoulos, Paschalis, and Dimitrios Koutsouris. "Sparsely activated networks." IEEE Transactions on Neural Networks and Learning Systems 32.3 (2020): 1304-1313.</p>
		<p id="citation-3">[3] Shannon, Claude Elwood. "A mathematical theory of communication." ACM SIGMOBILE mobile computing and communications review 5.1 (2001): 3-55.</p>
		<p id="citation-4">[4] Li, Ming, and Paul Vit√°nyi. An introduction to Kolmogorov complexity and its applications. Vol. 3. New York: Springer, 2008.</p>
		<p id="citation-5">[5] Chaitin, Gregory J. "Algorithmic information theory." IBM journal of research and development 21.4 (1977): 350-359.</p>
		<p id="citation-6">[6] Rissanen, Jorma. "A universal prior for integers and estimation by minimum description length." The Annals of statistics 11.2 (1983): 416-431.</p>
		<p id="citation-7">[7] Hutter, Marcus. "The human knowledge compression contest." URL http://prize. hutter1. net 6 (2012).</p>
	</body>
</html>
